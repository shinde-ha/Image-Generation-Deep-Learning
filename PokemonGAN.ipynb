{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Generation for MNIST using Deep Convolutional Progressive GAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Following code shows an implementation of DC-Progressive GAN for the Pokemon dataset. The code is designed in a single class for better execution of respective train and test functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.58 s\n"
     ]
    }
   ],
   "source": [
    "# Display time duration after execution of the cell\n",
    "%%time\n",
    "# Importing libraries\n",
    "from __future__ import print_function\n",
    "\n",
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling3D, Conv2D, Conv3D, UpSampling2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from scipy.misc import imsave as ims\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import sys\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from keras.models import model_from_json\n",
    "import utils\n",
    "\n",
    "save_interval = 200\n",
    "\n",
    "class DCGAN():\n",
    "    def __init__(self):\n",
    "        self.img_rows = 100\n",
    "        self.img_cols = 100\n",
    "        self.channels = 3\n",
    "\n",
    "        optimizer = Adam(0.0002, 0.5)\n",
    "\n",
    "        # Build and compile the discriminator\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy',\n",
    "            optimizer=optimizer,\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "        # Build and compile the generator\n",
    "        self.generator = self.build_generator()\n",
    "        self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        # The generator takes noise as input and generated imgs\n",
    "        z = Input(shape=(100,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        # For the combined model we will only train the generator\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        # The valid takes generated images as input and determines validity\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        # The combined model  (stacked generator and discriminator) takes\n",
    "        # noise as input => generates images => determines validity\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "\n",
    "        noise_shape = (100,)\n",
    "\n",
    "        model = Sequential()\n",
    "        # Generator upsampling is required for better random machine generation\n",
    "        model.add(Dense(128 * 25 * 25, activation=\"relu\", input_shape=noise_shape))\n",
    "        model.add(Reshape((25, 25, 128)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(3, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=noise_shape)\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "\n",
    "        img_shape = (self.img_rows, self.img_cols, self.channels)\n",
    "\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def train(self, epochs, batch_size):\n",
    "\n",
    "        # Load the dataset\n",
    "        X_train = utils.load_data()\n",
    "\n",
    "        # Rescale -1 to 1\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "        #X_train = np.expand_dims(X_train, axis=3)\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "\n",
    "        for epoch in range(epochs+1):\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Discriminator\n",
    "            # ---------------------\n",
    "\n",
    "            # Select a random half batch of images\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            # Sample noise and generate a half batch of new images\n",
    "            noise = np.random.normal(0, 1, (half_batch, 100))\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            # Train the discriminator (real classified as ones and generated as zeros)\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # ---------------------\n",
    "            #  Train Generator\n",
    "            # ---------------------\n",
    "\n",
    "            noise = np.random.normal(0, 1, (batch_size, 100))\n",
    "\n",
    "            # Train the generator (wants discriminator to mistake images as real)\n",
    "            g_loss = self.combined.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "            # Plot the progress\n",
    "            print (\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[1], g_loss))\n",
    "\n",
    "            if epoch == 0:\n",
    "                model_json = self.generator.to_json()\n",
    "                with open(\"weights/generator.json\", \"w\") as json_file:\n",
    "                    json_file.write(model_json)\n",
    "            # If at save interval => save generated image samples\n",
    "            if epoch % save_interval == 0:\n",
    "                self.save_imgs(epoch)\n",
    "                gen_name = \"weights/gen_\" + str(epoch) + \".h5\"\n",
    "                self.generator.save_weights(gen_name)\n",
    "\n",
    "    def save_imgs(self, epoch):\n",
    "        r, c = 3, 3\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # Rescale images 0 - 1\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "        #ims(\"images/pokemon_%d.png\" % epoch,utils.merge(gen_imgs,[3,3]))\n",
    "        ims('images/pokemon_%d.png'%epoch, utils.merge(gen_imgs,[3,3]))\n",
    "\n",
    "    def test_imgs(self):\n",
    "        r, c = 3, 3\n",
    "        noise = np.random.normal(0, 1, (r * c, 100))\n",
    "\n",
    "        # load json and create model\n",
    "        json_file = open('weights/generator.json', 'r')\n",
    "        loaded_model_json = json_file.read()\n",
    "        json_file.close()\n",
    "        loaded_model = model_from_json(loaded_model_json)\n",
    "        weightlist = glob.glob('weights/*.h5')\n",
    "        cnt = 0\n",
    "        for weight in weightlist:\n",
    "            # load weights into new model\n",
    "            loaded_model.load_weights(weight)\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "            # Rescale images 0 - 1\n",
    "            gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "            ims('images/test_pokemon_%d.png'%cnt, utils.merge(gen_imgs,[3,3]))\n",
    "            cnt = cnt+save_interval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    if not os.path.exists('images/'):\n",
    "        os.makedirs('images/')\n",
    "\n",
    "    if not os.path.exists('weights/'):\n",
    "        os.makedirs('weights/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 50, 50, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 50, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 50, 50, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 25, 25, 64)        18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d_1 (ZeroPaddin (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 26, 26, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 26, 26, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 13, 13, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 13, 13, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 13, 13, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 13, 13, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 43264)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 43265     \n",
      "=================================================================\n",
      "Total params: 432,449\n",
      "Trainable params: 432,065\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 80000)             8080000   \n",
      "_________________________________________________________________\n",
      "reshape_1 (Reshape)          (None, 25, 25, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 25, 25, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 50, 50, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 50, 50, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50, 50, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 50, 50, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_2 (UpSampling2 (None, 100, 100, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 100, 100, 64)      73792     \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 100, 100, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 100, 100, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 100, 100, 3)       1731      \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 100, 100, 3)       0         \n",
      "=================================================================\n",
      "Total params: 8,304,387\n",
      "Trainable params: 8,303,747\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "(151, 100, 100, 3)\n",
      "0 [D loss: 1.478354, acc.: 37.50%] [G loss: 0.545122]\n",
      "1 [D loss: 0.816658, acc.: 50.00%] [G loss: 1.041465]\n",
      "2 [D loss: 0.531631, acc.: 87.50%] [G loss: 2.004293]\n",
      "3 [D loss: 0.702574, acc.: 56.25%] [G loss: 1.557003]\n",
      "4 [D loss: 0.336207, acc.: 93.75%] [G loss: 1.372470]\n",
      "5 [D loss: 0.386116, acc.: 87.50%] [G loss: 1.633803]\n",
      "6 [D loss: 0.383605, acc.: 81.25%] [G loss: 1.987330]\n",
      "7 [D loss: 0.430598, acc.: 81.25%] [G loss: 2.248414]\n",
      "8 [D loss: 0.314034, acc.: 87.50%] [G loss: 1.871591]\n",
      "9 [D loss: 0.190793, acc.: 93.75%] [G loss: 2.119596]\n",
      "10 [D loss: 0.370438, acc.: 87.50%] [G loss: 1.887923]\n",
      "11 [D loss: 0.394234, acc.: 81.25%] [G loss: 3.150140]\n",
      "12 [D loss: 0.407651, acc.: 81.25%] [G loss: 2.205590]\n",
      "13 [D loss: 0.410231, acc.: 81.25%] [G loss: 1.907187]\n",
      "14 [D loss: 0.492057, acc.: 75.00%] [G loss: 2.117015]\n",
      "15 [D loss: 0.185906, acc.: 100.00%] [G loss: 3.229722]\n",
      "16 [D loss: 0.582665, acc.: 62.50%] [G loss: 1.474240]\n",
      "17 [D loss: 0.367210, acc.: 75.00%] [G loss: 3.232003]\n",
      "18 [D loss: 0.257870, acc.: 87.50%] [G loss: 3.985659]\n",
      "19 [D loss: 0.514418, acc.: 75.00%] [G loss: 2.557513]\n",
      "20 [D loss: 0.378128, acc.: 81.25%] [G loss: 3.287706]\n",
      "21 [D loss: 0.318541, acc.: 87.50%] [G loss: 3.475685]\n",
      "22 [D loss: 0.220572, acc.: 93.75%] [G loss: 2.921742]\n",
      "23 [D loss: 0.076504, acc.: 100.00%] [G loss: 2.737427]\n",
      "24 [D loss: 0.189386, acc.: 93.75%] [G loss: 2.719191]\n",
      "25 [D loss: 0.233800, acc.: 87.50%] [G loss: 2.066109]\n",
      "26 [D loss: 0.088800, acc.: 100.00%] [G loss: 2.647165]\n",
      "27 [D loss: 0.109893, acc.: 100.00%] [G loss: 3.107799]\n",
      "28 [D loss: 0.045262, acc.: 100.00%] [G loss: 3.321769]\n",
      "29 [D loss: 0.085750, acc.: 100.00%] [G loss: 2.660344]\n",
      "30 [D loss: 0.128858, acc.: 93.75%] [G loss: 3.746315]\n",
      "31 [D loss: 0.245326, acc.: 93.75%] [G loss: 2.469065]\n",
      "32 [D loss: 0.126756, acc.: 100.00%] [G loss: 2.903769]\n",
      "33 [D loss: 0.163864, acc.: 100.00%] [G loss: 3.399292]\n",
      "34 [D loss: 0.098494, acc.: 100.00%] [G loss: 3.468696]\n",
      "35 [D loss: 0.162285, acc.: 93.75%] [G loss: 3.050699]\n",
      "36 [D loss: 0.267360, acc.: 87.50%] [G loss: 3.393835]\n",
      "37 [D loss: 0.055073, acc.: 100.00%] [G loss: 4.235399]\n",
      "38 [D loss: 0.031092, acc.: 100.00%] [G loss: 4.158494]\n",
      "39 [D loss: 0.150262, acc.: 87.50%] [G loss: 4.222564]\n",
      "40 [D loss: 0.484582, acc.: 75.00%] [G loss: 4.979054]\n",
      "41 [D loss: 0.041393, acc.: 100.00%] [G loss: 5.688745]\n",
      "42 [D loss: 0.102070, acc.: 93.75%] [G loss: 3.968746]\n",
      "43 [D loss: 0.320501, acc.: 87.50%] [G loss: 3.772506]\n",
      "44 [D loss: 0.018662, acc.: 100.00%] [G loss: 6.092927]\n",
      "45 [D loss: 0.119899, acc.: 93.75%] [G loss: 3.366346]\n",
      "46 [D loss: 0.019358, acc.: 100.00%] [G loss: 3.398018]\n",
      "47 [D loss: 0.191590, acc.: 93.75%] [G loss: 6.400863]\n",
      "48 [D loss: 0.130283, acc.: 93.75%] [G loss: 5.683620]\n",
      "49 [D loss: 0.316224, acc.: 81.25%] [G loss: 3.427188]\n",
      "50 [D loss: 0.053980, acc.: 100.00%] [G loss: 5.891168]\n",
      "51 [D loss: 0.024069, acc.: 100.00%] [G loss: 5.255769]\n",
      "52 [D loss: 0.015102, acc.: 100.00%] [G loss: 4.396427]\n",
      "53 [D loss: 0.008139, acc.: 100.00%] [G loss: 4.139806]\n",
      "54 [D loss: 0.056746, acc.: 100.00%] [G loss: 4.594489]\n",
      "55 [D loss: 0.174645, acc.: 87.50%] [G loss: 4.420281]\n",
      "56 [D loss: 0.280023, acc.: 81.25%] [G loss: 5.957744]\n",
      "57 [D loss: 0.006804, acc.: 100.00%] [G loss: 8.476280]\n",
      "58 [D loss: 0.982200, acc.: 50.00%] [G loss: 2.086583]\n",
      "59 [D loss: 0.212478, acc.: 93.75%] [G loss: 6.466010]\n",
      "60 [D loss: 0.008690, acc.: 100.00%] [G loss: 9.288553]\n",
      "61 [D loss: 1.668649, acc.: 18.75%] [G loss: 7.693641]\n",
      "62 [D loss: 0.046193, acc.: 93.75%] [G loss: 10.106110]\n",
      "63 [D loss: 0.437331, acc.: 68.75%] [G loss: 1.743883]\n",
      "64 [D loss: 0.232168, acc.: 93.75%] [G loss: 2.457172]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65 [D loss: 0.059893, acc.: 100.00%] [G loss: 3.829294]\n",
      "66 [D loss: 0.024061, acc.: 100.00%] [G loss: 4.524200]\n",
      "67 [D loss: 0.019704, acc.: 100.00%] [G loss: 5.230810]\n",
      "68 [D loss: 0.008194, acc.: 100.00%] [G loss: 5.467308]\n",
      "69 [D loss: 0.043568, acc.: 100.00%] [G loss: 3.222866]\n",
      "70 [D loss: 0.090147, acc.: 100.00%] [G loss: 2.780261]\n",
      "71 [D loss: 0.143623, acc.: 93.75%] [G loss: 6.451217]\n",
      "72 [D loss: 0.227672, acc.: 93.75%] [G loss: 3.165343]\n",
      "73 [D loss: 0.026552, acc.: 100.00%] [G loss: 4.292233]\n",
      "74 [D loss: 0.142790, acc.: 100.00%] [G loss: 5.736993]\n",
      "75 [D loss: 0.166926, acc.: 87.50%] [G loss: 3.787616]\n",
      "76 [D loss: 0.174505, acc.: 100.00%] [G loss: 2.152539]\n",
      "77 [D loss: 0.108763, acc.: 93.75%] [G loss: 7.493873]\n",
      "78 [D loss: 0.591238, acc.: 81.25%] [G loss: 0.132409]\n",
      "79 [D loss: 1.178607, acc.: 50.00%] [G loss: 12.839725]\n",
      "80 [D loss: 1.063779, acc.: 62.50%] [G loss: 8.024033]\n",
      "81 [D loss: 0.169318, acc.: 100.00%] [G loss: 5.849035]\n",
      "82 [D loss: 1.111605, acc.: 50.00%] [G loss: 10.272861]\n",
      "83 [D loss: 0.165481, acc.: 93.75%] [G loss: 9.702690]\n",
      "84 [D loss: 1.655653, acc.: 43.75%] [G loss: 10.053566]\n",
      "85 [D loss: 1.385368, acc.: 62.50%] [G loss: 4.723959]\n",
      "86 [D loss: 1.648655, acc.: 43.75%] [G loss: 14.474958]\n",
      "87 [D loss: 2.621264, acc.: 25.00%] [G loss: 4.185391]\n",
      "88 [D loss: 0.328054, acc.: 87.50%] [G loss: 6.477715]\n",
      "89 [D loss: 0.767945, acc.: 56.25%] [G loss: 4.002404]\n",
      "90 [D loss: 0.943891, acc.: 50.00%] [G loss: 5.317715]\n",
      "91 [D loss: 0.446756, acc.: 75.00%] [G loss: 6.500055]\n",
      "92 [D loss: 0.420506, acc.: 87.50%] [G loss: 1.033147]\n",
      "93 [D loss: 1.218305, acc.: 56.25%] [G loss: 11.124014]\n",
      "94 [D loss: 0.761738, acc.: 75.00%] [G loss: 3.736467]\n",
      "95 [D loss: 0.635908, acc.: 81.25%] [G loss: 3.729305]\n",
      "96 [D loss: 0.925527, acc.: 68.75%] [G loss: 4.814853]\n",
      "97 [D loss: 3.216383, acc.: 37.50%] [G loss: 3.099465]\n",
      "98 [D loss: 0.092240, acc.: 93.75%] [G loss: 5.384374]\n",
      "99 [D loss: 1.745187, acc.: 50.00%] [G loss: 3.271651]\n",
      "100 [D loss: 1.175328, acc.: 43.75%] [G loss: 5.207057]\n",
      "101 [D loss: 4.251452, acc.: 0.00%] [G loss: 5.828610]\n",
      "102 [D loss: 1.497961, acc.: 37.50%] [G loss: 5.734738]\n",
      "103 [D loss: 3.009765, acc.: 6.25%] [G loss: 2.808405]\n",
      "104 [D loss: 1.744247, acc.: 56.25%] [G loss: 2.020475]\n",
      "105 [D loss: 0.883581, acc.: 62.50%] [G loss: 5.285524]\n",
      "106 [D loss: 0.919324, acc.: 50.00%] [G loss: 2.519315]\n",
      "107 [D loss: 0.917220, acc.: 68.75%] [G loss: 5.198758]\n",
      "108 [D loss: 0.704941, acc.: 75.00%] [G loss: 1.570524]\n",
      "109 [D loss: 1.895215, acc.: 56.25%] [G loss: 1.900135]\n",
      "110 [D loss: 0.584862, acc.: 75.00%] [G loss: 4.888518]\n",
      "111 [D loss: 2.715559, acc.: 31.25%] [G loss: 3.914096]\n",
      "112 [D loss: 3.665470, acc.: 0.00%] [G loss: 3.592746]\n",
      "113 [D loss: 2.093349, acc.: 37.50%] [G loss: 4.309685]\n",
      "114 [D loss: 2.787950, acc.: 18.75%] [G loss: 3.285192]\n",
      "115 [D loss: 1.791896, acc.: 25.00%] [G loss: 2.946448]\n",
      "116 [D loss: 1.151704, acc.: 62.50%] [G loss: 1.687368]\n",
      "117 [D loss: 1.460052, acc.: 37.50%] [G loss: 3.441903]\n",
      "118 [D loss: 1.451279, acc.: 50.00%] [G loss: 3.460226]\n",
      "119 [D loss: 2.559175, acc.: 0.00%] [G loss: 1.773522]\n",
      "120 [D loss: 1.238238, acc.: 50.00%] [G loss: 2.764447]\n",
      "121 [D loss: 0.700906, acc.: 68.75%] [G loss: 3.486659]\n",
      "122 [D loss: 0.947822, acc.: 56.25%] [G loss: 1.724820]\n",
      "123 [D loss: 2.050362, acc.: 12.50%] [G loss: 3.415126]\n",
      "124 [D loss: 1.153000, acc.: 50.00%] [G loss: 2.995473]\n",
      "125 [D loss: 1.072905, acc.: 50.00%] [G loss: 2.424244]\n",
      "126 [D loss: 1.440428, acc.: 37.50%] [G loss: 1.462602]\n",
      "127 [D loss: 1.997219, acc.: 37.50%] [G loss: 3.124749]\n",
      "128 [D loss: 1.328362, acc.: 50.00%] [G loss: 3.377351]\n",
      "129 [D loss: 1.664349, acc.: 31.25%] [G loss: 2.920524]\n",
      "130 [D loss: 1.508713, acc.: 56.25%] [G loss: 3.967523]\n",
      "131 [D loss: 2.283873, acc.: 31.25%] [G loss: 2.038141]\n",
      "132 [D loss: 2.007339, acc.: 43.75%] [G loss: 2.422461]\n",
      "133 [D loss: 1.901432, acc.: 31.25%] [G loss: 4.278136]\n",
      "134 [D loss: 2.597546, acc.: 18.75%] [G loss: 2.403111]\n",
      "135 [D loss: 3.532665, acc.: 12.50%] [G loss: 3.388171]\n",
      "136 [D loss: 1.893168, acc.: 18.75%] [G loss: 1.731310]\n",
      "137 [D loss: 0.995097, acc.: 43.75%] [G loss: 4.138667]\n",
      "138 [D loss: 1.383930, acc.: 43.75%] [G loss: 1.837124]\n",
      "139 [D loss: 1.458822, acc.: 37.50%] [G loss: 2.129430]\n",
      "140 [D loss: 1.552346, acc.: 31.25%] [G loss: 4.571297]\n",
      "141 [D loss: 1.564376, acc.: 25.00%] [G loss: 3.141179]\n",
      "142 [D loss: 1.723121, acc.: 18.75%] [G loss: 3.815953]\n",
      "143 [D loss: 1.758389, acc.: 31.25%] [G loss: 2.797067]\n",
      "144 [D loss: 1.953930, acc.: 43.75%] [G loss: 3.386602]\n",
      "145 [D loss: 1.437506, acc.: 37.50%] [G loss: 2.626808]\n",
      "146 [D loss: 0.890534, acc.: 50.00%] [G loss: 1.657152]\n",
      "147 [D loss: 0.694988, acc.: 68.75%] [G loss: 2.047280]\n",
      "148 [D loss: 1.660512, acc.: 43.75%] [G loss: 2.870123]\n",
      "149 [D loss: 0.733133, acc.: 62.50%] [G loss: 2.509644]\n",
      "150 [D loss: 1.587073, acc.: 31.25%] [G loss: 2.072771]\n",
      "151 [D loss: 1.285462, acc.: 25.00%] [G loss: 2.040249]\n",
      "152 [D loss: 1.153523, acc.: 37.50%] [G loss: 2.885940]\n",
      "153 [D loss: 0.850069, acc.: 68.75%] [G loss: 2.233258]\n",
      "154 [D loss: 1.109576, acc.: 62.50%] [G loss: 4.179767]\n",
      "155 [D loss: 3.045742, acc.: 0.00%] [G loss: 1.863590]\n",
      "156 [D loss: 2.357366, acc.: 25.00%] [G loss: 4.462725]\n",
      "157 [D loss: 1.737822, acc.: 25.00%] [G loss: 1.754509]\n",
      "158 [D loss: 1.985108, acc.: 18.75%] [G loss: 1.347641]\n",
      "159 [D loss: 0.907134, acc.: 56.25%] [G loss: 2.267302]\n",
      "160 [D loss: 1.380802, acc.: 56.25%] [G loss: 2.499383]\n",
      "161 [D loss: 1.486677, acc.: 43.75%] [G loss: 1.716094]\n",
      "162 [D loss: 2.112084, acc.: 18.75%] [G loss: 2.093624]\n",
      "163 [D loss: 1.548871, acc.: 31.25%] [G loss: 2.854913]\n",
      "164 [D loss: 0.780224, acc.: 75.00%] [G loss: 4.244966]\n",
      "165 [D loss: 2.103483, acc.: 31.25%] [G loss: 3.122661]\n",
      "166 [D loss: 3.457904, acc.: 12.50%] [G loss: 2.544331]\n",
      "167 [D loss: 1.482376, acc.: 43.75%] [G loss: 3.966040]\n",
      "168 [D loss: 1.571787, acc.: 31.25%] [G loss: 2.574459]\n",
      "169 [D loss: 1.498375, acc.: 43.75%] [G loss: 2.112602]\n",
      "170 [D loss: 1.166307, acc.: 62.50%] [G loss: 4.183867]\n",
      "171 [D loss: 2.348886, acc.: 18.75%] [G loss: 1.943991]\n",
      "172 [D loss: 1.874437, acc.: 37.50%] [G loss: 3.178419]\n",
      "173 [D loss: 1.278962, acc.: 56.25%] [G loss: 2.527136]\n",
      "174 [D loss: 1.615836, acc.: 37.50%] [G loss: 2.609389]\n",
      "175 [D loss: 1.685817, acc.: 31.25%] [G loss: 2.431026]\n",
      "176 [D loss: 2.440679, acc.: 31.25%] [G loss: 2.383098]\n",
      "177 [D loss: 2.176089, acc.: 12.50%] [G loss: 1.454370]\n",
      "178 [D loss: 1.352982, acc.: 62.50%] [G loss: 3.670348]\n",
      "179 [D loss: 1.783512, acc.: 31.25%] [G loss: 1.471329]\n",
      "180 [D loss: 2.637531, acc.: 12.50%] [G loss: 2.355463]\n",
      "181 [D loss: 1.954025, acc.: 25.00%] [G loss: 3.554689]\n",
      "182 [D loss: 2.444599, acc.: 12.50%] [G loss: 1.576559]\n",
      "183 [D loss: 2.871761, acc.: 6.25%] [G loss: 2.040677]\n",
      "184 [D loss: 1.489011, acc.: 31.25%] [G loss: 2.568405]\n",
      "185 [D loss: 2.770658, acc.: 12.50%] [G loss: 1.018572]\n",
      "186 [D loss: 1.465161, acc.: 25.00%] [G loss: 1.593221]\n",
      "187 [D loss: 1.708436, acc.: 50.00%] [G loss: 2.535825]\n",
      "188 [D loss: 1.708545, acc.: 50.00%] [G loss: 2.406410]\n",
      "189 [D loss: 1.695117, acc.: 18.75%] [G loss: 0.807393]\n",
      "190 [D loss: 2.004663, acc.: 25.00%] [G loss: 2.629401]\n",
      "191 [D loss: 3.236163, acc.: 0.00%] [G loss: 2.573130]\n",
      "192 [D loss: 2.644011, acc.: 25.00%] [G loss: 1.949736]\n",
      "193 [D loss: 2.339463, acc.: 31.25%] [G loss: 2.098220]\n",
      "194 [D loss: 2.377354, acc.: 25.00%] [G loss: 3.772815]\n",
      "195 [D loss: 2.130603, acc.: 25.00%] [G loss: 2.975825]\n",
      "196 [D loss: 1.937504, acc.: 25.00%] [G loss: 4.499449]\n",
      "197 [D loss: 1.882364, acc.: 31.25%] [G loss: 1.455204]\n",
      "198 [D loss: 1.363154, acc.: 50.00%] [G loss: 4.569072]\n",
      "199 [D loss: 1.959937, acc.: 31.25%] [G loss: 2.697804]\n",
      "200 [D loss: 2.855075, acc.: 18.75%] [G loss: 1.627699]\n",
      "Wall time: 26min 39s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dcgan = DCGAN()\n",
    "#call train function for network initialization and model training\n",
    "dcgan.train(epochs=200, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check the respective weights and generate images to save it in images directory\n",
    "dcgan.test_imgs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
